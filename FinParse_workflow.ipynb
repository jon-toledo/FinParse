{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinParse:  A customized parser for financial resumes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a walk-through of an NLP based resume parser.  The NLP models are trained on resumes from finance and accounting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# utility modules\n",
    "import importdata\n",
    "import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data comes from ~1000 pre-parsed resumes.  Due to privacy issues I cannot make this data publicly available and so the user will not be able to train the models in the following sections.  I hope the notebook is still understandable and useful in the case that someone has their own training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading testing and training data\n",
    "data_train, data_test = importdata.extract_data('../../data/pdf_resume_data.csv')\n",
    "\n",
    "emp_name_tr, pos_name_tr, edu_lines_tr, descrips_tr, headers_tr = map(parse.kill_space, data_train)\n",
    "emp_name_te, pos_name_te, edu_lines_te, descrips_te, headers_te = map(parse.kill_space, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each part of the dataset is a list of strings corresponding to a given category.  For example ```emp_name_tr``` schematically looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```['Some Business Inc', 'Another Business Ltd.',...  ]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ```headers_tr``` and ```headers_te``` datasets are slightly different in form.  Schematically they look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```[['header 1', 'header 1 classification'], ['header 2', 'header 2 classification'], ...]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of each pair is the actual header from the parsed resume while the second element is the classification of the header assigned by the parser.  These form the [X,y] pairs for the training and testing of the header classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will also be useful to have some resumes to test/demonstrate various parts of the notebook.  For that purpose I will use two resumes that I found on line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example resumes (publicly available)\n",
    "res_ex1 = importdata.pdf_to_text('example_resumes/Professional Finance Resume Format.pdf')\n",
    "res_ex2 = importdata.pdf_to_text('example_resumes/Finance Executive Assistant Resume.pdf')\n",
    "res_ex3 = importdata.pdf_to_text('example_resumes/Jon_Toledo_Final.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic strategy is to pass the data on each line of a resume through two levels of classifiers.  The classifier tries to identify if a resume line corresponds to a header or not.  If the line is identified as a header is it passed through a second level of classification to identify which type of header it is (Work experience, Education, Certification, Other).  If the line is not identified as a header it is passed on for later processing.  Using the header information the resume sections related to work experience and education are extracted.  Finally, each line in the work experience section is passed through a final filter to identify if it corresponds to an employer, role or description.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first classifier I train classifies each line as ```['EMPL', 'TITL', 'EDUC','DESC', 'HEAD']```.  What I am really interested in is if the line is a header or not.  For this classification task I find the best results using a linear SVC as the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "line_bags_tr = [emp_name_tr, pos_name_tr, edu_lines_tr, descrips_tr, list(zip(*headers_tr)[0])]\n",
    "\n",
    "# Prepare classifier for fitting\n",
    "model_lines = CalibratedClassifierCV(LinearSVC(random_state=0, tol=1e-5, penalty='l1', dual=False)) \n",
    "\n",
    "# Instantiate the classifier\n",
    "clf_lines = parse.Classifier(line_bags_tr, ['EMPL', 'TITL', 'EDUC','DESC', 'HEAD'], model_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.96      0.94       414\n",
      "          1       0.98      0.97      0.98       400\n",
      "          2       0.96      0.93      0.95       472\n",
      "          3       0.98      0.96      0.97       397\n",
      "          4       0.98      0.99      0.98       439\n",
      "\n",
      "avg / total       0.96      0.96      0.96      2122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can check the performance of the classifier using the test data\n",
    "\n",
    "line_bags_te = [emp_name_te, pos_name_te, edu_lines_te, descrips_te, list(zip(*headers_te)[0])]\n",
    "print clf_lines.score_report(line_bags_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how it works on a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('HEAD', array([[0.00465499, 0.02024124, 0.2638527 , 0.01387927, 0.6973718 ]]))\n"
     ]
    }
   ],
   "source": [
    "print clf_lines.text_class(['CERTIFICATION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of the tuple is the predicted class.  The second element of the tuple is the probability that it belongs to each of the various classes.  In this case the text has been identified as a header.  To give another example, consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TITL', array([[2.92355655e-06, 9.93187579e-01, 3.75972349e-04, 2.88223569e-04,\n",
      "        6.14530103e-03]]))\n"
     ]
    }
   ],
   "source": [
    "print clf_lines.text_class(['ANALYST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have identified the headers, I train another model to classify each header as ```['HEAD_WORK', 'HEAD_EDUC','HEAD_OTHR']```.   First we need to massage the data into the correct form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WORK HISTORY                              0\n",
       "EDUCATION                                 1\n",
       "SUMMARY                                   2\n",
       "SPEAKING                                  2\n",
       "SKILLS                                    2\n",
       "SECURITY_CLEARANCES                       2\n",
       "REFERENCES                                2\n",
       "QUALIFICATIONS_SUMMARY                    2\n",
       "PROJECT_HEADERS                           2\n",
       "PROFESSIONAL AFFILIATIONS                 2\n",
       "PERSONAL INTERESTS AND ACCOMPLISHMENTS    2\n",
       "ARTICLES                                  2\n",
       "OTHER_PUBLICATIONS                        2\n",
       "OBJECTIVE                                 2\n",
       "LICENSES                                  2\n",
       "LANGUAGES                                 2\n",
       "HOBBIES                                   2\n",
       "HEADERS_TO_IGNORE                         2\n",
       "CONTACT INFO                              2\n",
       "CERTIFICATIONS                            2\n",
       "TRAINING                                  2\n",
       "PATENTS                                   2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary which maps 'WORK HISTORY': 0, 'EDUCATION': 1, all other classes: 2. \n",
    "header_cats = ['HEAD_WORK', 'HEAD_EDUC','HEAD_OTHR']\n",
    "wanted_header = ['WORK HISTORY', 'EDUCATION']\n",
    "header_dict = {}\n",
    "for header in set([ header[1] for header in headers_tr ]):\n",
    "    if header not in wanted_header:\n",
    "        header_dict[header] = len(header_cats)-1\n",
    "    else:\n",
    "        header_dict[header] = wanted_header.index(header)\n",
    "    \n",
    "pd.Series(header_dict).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[161, 146, 462]\n"
     ]
    }
   ],
   "source": [
    "# regroup the header_tr data into standard form for input into Classifier\n",
    "header_bags_tr = [[],[],[]]\n",
    "for el in headers_tr:\n",
    "    if el[0] not in header_bags_tr[header_dict[el[1]]]:\n",
    "        header_bags_tr[header_dict[el[1]]].append(el[0])\n",
    "        \n",
    "# print the size of each bag\n",
    "print map(len, header_bags_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the header classifer\n",
    "model_headers = CalibratedClassifierCV(LinearSVC(random_state=0, tol=1e-5, penalty='l1', dual=False)) \n",
    "clf_headers = parse.Classifier(header_bags_tr, ['HEAD_WORK', 'HEAD_EDUC','HEAD_OTHR'], model_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.89      0.93        38\n",
      "          1       0.93      0.90      0.92        31\n",
      "          2       0.94      0.98      0.96       123\n",
      "\n",
      "avg / total       0.95      0.95      0.95       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regroup the header_te data into standard form for input into Classifier\n",
    "header_bags_te = [[],[],[]]\n",
    "for el in headers_te:\n",
    "    if el[0] not in header_bags_te[header_dict[el[1]]]:\n",
    "        header_bags_te[header_dict[el[1]]].append(el[0])\n",
    "        \n",
    "# test the performance of the header classifer \n",
    "print clf_headers.score_report(header_bags_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this classifier works in a few examples, consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HEAD_WORK', array([[0.96621902, 0.02725392, 0.00652706]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_headers.text_class(['Work Experience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HEAD_OTHR', array([[0.08856464, 0.0282039 , 0.88323146]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_headers.text_class(['Volunteer Experience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('UNKN', array([0, 0, 0, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_headers.text_class(['Hello World'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work section classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train a classifier to identify lines within the work section of a resume as ```['EMPL', 'TITL', 'DESC']```.  For this task I found the best overall performance with a multinomial naive Bayes classifier.  (Actually, the performance on the testing data was still the best with the linear SVC, however the performance when inputing actual lines from resumes was the best with the NB model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.89      0.94       414\n",
      "          1       0.97      0.97      0.97       400\n",
      "          2       0.90      0.99      0.94       397\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_wrk = parse.Classifier([emp_name_tr, pos_name_tr, descrips_tr], ['EMPL', 'TITL', 'DESC'], MultinomialNB())\n",
    "\n",
    "print clf_wrk.score_report([emp_name_te, pos_name_te, descrips_te])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this classifier works in some examples, consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TITL', array([[0.00248455, 0.98595934, 0.01155611]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_wrk.text_class(['Analyst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('EMPL', array([[0.50754983, 0.24818616, 0.24426401]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_wrk.text_class(['Hello World Traders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DESC', array([[0.2012993 , 0.04120023, 0.75750047]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_wrk.text_class(['I worked for Hello World Traders Inc. for seven years'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to begin parsing resumes!  As I show in the next section, the parsing of a resume end-to-end can be done in one line using the functions in the ```parse``` module.  In this section we give a walk-through of the process that is going on under the hood.  The process involves four steps:\n",
    "\n",
    "1. Breaking into 'lines'\n",
    "2. Extracting sections\n",
    "3. Classifying lines within sections\n",
    "4. Clustering related information\n",
    "\n",
    "I will demonstrate each of these in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking into lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider our first example resume ```res_ex1```.  We can have a look at a portion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "utilities, and related community services, with $60 million in annual revenues and 700 full-time employees. \n",
      "Senior Budget Analyst (2005-Present) \n",
      "Management Analyst (2004-2005) \n",
      "Progressed rapidly to Senior Budget Analyst to manage Performance Measurement and Accountability system across \n",
      "60 government departments and programs. Conduct budget, revenue, and variance / trend monitoring and analysis of \n",
      "performance and operational results, and provide associated semi-annual reports to government\n"
     ]
    }
   ],
   "source": [
    "print res_ex1[2000:2500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy I use for breaking the resume into 'lines' is to split the text on ```'\\n'``` and then on ```'\\s{5,}'```.  So more specifically, I break the text into lines and then into chunks of text on each line separated by 5 or more spaces.  For example the portion of ```res_ex1``` above gets broken down into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ']\n",
      "['utilities, and related community services, with $60 million in annual revenues and 700 full-time employees. ']\n",
      "['Senior Budget Analyst (2005-Present) ']\n",
      "['Management Analyst (2004-2005) ']\n",
      "['Progressed rapidly to Senior Budget Analyst to manage Performance Measurement and Accountability system across ']\n",
      "['60 government departments and programs. Conduct budget, revenue, and variance / trend monitoring and analysis of ']\n",
      "['performance and operational results, and provide associated semi-annual reports to government']\n"
     ]
    }
   ],
   "source": [
    "for lis in parse.extract_lines(res_ex1[2000:2500]):\n",
    "    print lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we give a final classification of the lines, we first break the resume into sections using the header classification.  This allows us to classify the lines within each section using a more refined model and thus reduces the number of mistakes.  For example, we can extract the work section of ```res_ex1``` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['HEAD_OTHR', 10], ['HEAD_WORK', 26], ['HEAD_EDUC', 62], ['HEAD_OTHR', 70], ['HEAD_OTHR', 75], ['HEAD_OTHR', 78]]\n"
     ]
    }
   ],
   "source": [
    "# some basic cleaning of the resume text (e.g. identifying and tagging dates using regex)\n",
    "res_ex1_cleaned = parse.clean_resume(res_ex1)\n",
    "\n",
    "# extract the line position of each header\n",
    "headers_res_ex1 = parse.extract_headers(res_ex1_cleaned, clf_lines.text_class, clf_headers.text_class)\n",
    "print headers_res_ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROFESSIONAL EXPERIENCE \n",
      "COUNTY OF SONOMA –  Sonoma, CA  \n",
      "  \n",
      "~DATE(S)~: 2004-Present\n",
      "Government Agency responsible for administration of public works, law enforcement, public safety, el\n",
      "Senior Budget Analyst  \n",
      "~DATE(S)~: 2005-Present\n",
      "Management Analyst  \n",
      "~DATE(S)~: 2004-2005\n",
      "Progressed rapidly to Senior Budget Analyst to manage Performance Measurement and Accountability sys\n"
     ]
    }
   ],
   "source": [
    "# extract the work section of res_ex1\n",
    "wrk_res_ex1 = parse.extract_section(res_ex1_cleaned, headers_res_ex1 ,'HEAD_WORK')\n",
    "\n",
    "# printing the first ten lines of the work section of res_ex1\n",
    "for line in wrk_res_ex1[0:10]:\n",
    "    print line[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying lines within a section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting the work section we then classify each line as ```['EMPL', 'TITL', 'DESC']```.  If the line is labeled ```UNKN``` this means that it does not contain any words in the classifiers vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPL...0.51 <== COUNTY OF SONOMA –  Sonoma, CA  \n",
      "\n",
      "UNKN...0.0 <==   \n",
      "\n",
      "DATE...1.0 <== 2004-Present\n",
      "\n",
      "DESC...0.98 <== Government Agency responsible for administration of public works, law enforcement, public safety, el\n",
      "\n",
      "TITL...0.96 <== Senior Budget Analyst  \n",
      "\n",
      "DATE...1.0 <== 2005-Present\n",
      "\n",
      "TITL...0.93 <== Management Analyst  \n",
      "\n",
      "DATE...1.0 <== 2004-2005\n",
      "\n",
      "DESC...0.86 <== Progressed rapidly to Senior Budget Analyst to manage Performance Measurement and Accountability sys\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classify a few lines in work section of res_ex1.  The print out is in the form:\n",
    "\n",
    "#       CLASS...probablility of class <== line that was classified \n",
    "\n",
    "clfd_wrk_lines = parse.clf_intra_sec(res_ex1, clf_lines, clf_headers, clf_wrk)\n",
    "for line in clfd_wrk_lines[1:10]:\n",
    "    print line[0] + '...' + str(line[1]) + ' <== ' +  line[-1][0:100] + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPL...0.65 <== Acme International Group – Phoenix, Arizona & Las Vegas, Nevada  \n",
      "\n",
      "UNKN...0.0 <==  \n",
      "\n",
      "DATE...1.0 <== 2010 to Present\n",
      "\n",
      "TITL...0.69 <== EXECUTIVE ASSISTANT TO CFO & DIRECTOR OF HUMAN RESOURCES \n",
      "\n",
      "DESC...1.0 <== Deliver  firsthand  support  to  senior  leaders  and  decision  makers  while  managing  a  variety\n",
      "\n",
      "DESC...0.98 <== Texas office, and distribution of B-5s to Acme partners. Coordinate Board packages for meetings—ga\n",
      "\n",
      "DESC...0.67 <== Internationally-based Finance team. \n",
      "\n",
      "DESC...1.0 <== (cid:1)  Following departure of CFO, transitioned from focused position to more comprehensive suppor\n",
      "\n",
      "DESC...0.98 <== (cid:1)  Overhauled and vastly improved attendance management by researching, selecting, and driving\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repeat classification of a few lines in the work section of res_ex2\n",
    "clfd_wrk_lines = parse.clf_intra_sec(res_ex2, clf_lines, clf_headers, clf_wrk)\n",
    "for line in clfd_wrk_lines[1:10]:\n",
    "    print line[0] + '...' + str(line[1]) + ' <== ' +  line[-1][0:100] + '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clustering lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of the parsing process is to cluster together related information such as a title of a role with a particular employer and a date.  For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('EMPL', 'COUNTY OF SONOMA \\xe2\\x80\\x93  Sonoma, CA  '),\n",
       "              ('TITLS',\n",
       "               [OrderedDict([('TITL', 'Senior Budget Analyst  '),\n",
       "                             ('DATE', '2005-Present')]),\n",
       "                OrderedDict([('TITL', 'Management Analyst  '),\n",
       "                             ('DATE', '2004-2005')])])]),\n",
       " OrderedDict([('EMPL',\n",
       "               'LASER SOLUTIONS, INC (Wholly owned subsidiary of Digital Imprints, Ltd.) \\xe2\\x80\\x93 Athens, GA  '),\n",
       "              ('TITLS',\n",
       "               [OrderedDict([('TITL', 'Business Analysis Manager  '),\n",
       "                             ('DATE', '2000-2003')])])])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse.cluster_lines(parse.clf_intra_sec(res_ex1, clf_lines, clf_headers, clf_wrk), 'HEAD_WORK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('EMPL',\n",
       "               'Acme International Group \\xe2\\x80\\x93 Phoenix, Arizona & Las Vegas, Nevada  '),\n",
       "              ('TITLS',\n",
       "               [OrderedDict([('TITL',\n",
       "                              'EXECUTIVE ASSISTANT TO CFO & DIRECTOR OF HUMAN RESOURCES '),\n",
       "                             ('DATE', '2010 to Present')])])]),\n",
       " OrderedDict([('EMPL', 'Blue Hill Enterprises \\xe2\\x80\\x93 Phoenix, Arizona '),\n",
       "              ('TITLS',\n",
       "               [OrderedDict([('TITL', 'EXECUTIVE ASSISTANT TO CEO '),\n",
       "                             ('DATE', '1998 to 2010')])])]),\n",
       " OrderedDict([('EMPL', 'River Hills Inc. \\xe2\\x80\\x93 Phoenix, Arizona '),\n",
       "              ('TITLS',\n",
       "               [OrderedDict([('TITL',\n",
       "                              'EXECUTIVE ASSISTANT TO MANAGING PARTNER '),\n",
       "                             ('DATE', '1995 to 1998')])])]),\n",
       " OrderedDict([('EMPL', 'Weaver Law Firm \\xe2\\x80\\x93 Phoenix, Arizona '),\n",
       "              ('TITLS',\n",
       "               [OrderedDict([('TITL', 'OFFICE MANAGER '),\n",
       "                             ('DATE', '1993 to 1995')])])])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse.cluster_lines(parse.clf_intra_sec(res_ex2, clf_lines, clf_headers, clf_wrk), 'HEAD_WORK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we demonstrate the full parser on the two example resumes.  The output is a list of dictionaries.  One can compare with the pdf versions of the two examples to see that all the work information is correctly parsed (see the example_resumes folder in repo).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"7d6a0d05-4b59-42e5-8017-2ed0d6e04194\" style=\"height: 600px; width:100%;\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
       "          document.getElementById('7d6a0d05-4b59-42e5-8017-2ed0d6e04194').appendChild(renderjson({\"WORK HISTORY\": [{\"EMPL\": \"COUNTY OF SONOMA \\u2013  Sonoma, CA  \", \"TITLS\": [{\"TITL\": \"Senior Budget Analyst  \", \"DATE\": \"2005-Present\"}, {\"TITL\": \"Management Analyst  \", \"DATE\": \"2004-2005\"}]}, {\"EMPL\": \"LASER SOLUTIONS, INC (Wholly owned subsidiary of Digital Imprints, Ltd.) \\u2013 Athens, GA  \", \"TITLS\": [{\"TITL\": \"Business Analysis Manager  \", \"DATE\": \"2000-2003\"}]}]}))\n",
       "        });\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parse.RenderJSON({'WORK HISTORY': parse.cluster_lines(parse.clf_intra_sec(res_ex1, clf_lines, \\\n",
    "                                                                          clf_headers, clf_wrk), 'HEAD_WORK')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"4f36dd71-4603-4721-b36c-2c3e0d9f2e0f\" style=\"height: 600px; width:100%;\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
       "          document.getElementById('4f36dd71-4603-4721-b36c-2c3e0d9f2e0f').appendChild(renderjson({\"WORK HISTORY\": [{\"EMPL\": \"Acme International Group \\u2013 Phoenix, Arizona & Las Vegas, Nevada  \", \"TITLS\": [{\"TITL\": \"EXECUTIVE ASSISTANT TO CFO & DIRECTOR OF HUMAN RESOURCES \", \"DATE\": \"2010 to Present\"}]}, {\"EMPL\": \"Blue Hill Enterprises \\u2013 Phoenix, Arizona \", \"TITLS\": [{\"TITL\": \"EXECUTIVE ASSISTANT TO CEO \", \"DATE\": \"1998 to 2010\"}]}, {\"EMPL\": \"River Hills Inc. \\u2013 Phoenix, Arizona \", \"TITLS\": [{\"TITL\": \"EXECUTIVE ASSISTANT TO MANAGING PARTNER \", \"DATE\": \"1995 to 1998\"}]}, {\"EMPL\": \"Weaver Law Firm \\u2013 Phoenix, Arizona \", \"TITLS\": [{\"TITL\": \"OFFICE MANAGER \", \"DATE\": \"1993 to 1995\"}]}]}))\n",
       "        });\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parse.RenderJSON({'WORK HISTORY': parse.cluster_lines(parse.clf_intra_sec(res_ex2, clf_lines, \\\n",
    "                                                                          clf_headers, clf_wrk), 'HEAD_WORK')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"9048f87b-f04d-4625-b5d1-4925ccf49673\" style=\"height: 600px; width:100%;\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        require([\"https://rawgit.com/caldwell/renderjson/master/renderjson.js\"], function() {\n",
       "          document.getElementById('9048f87b-f04d-4625-b5d1-4925ccf49673').appendChild(renderjson({\"WORK HISTORY\": [{\"EMPL\": \"Swiss Federal Institute of Technology Lausanne\", \"TITLS\": [{\"TITL\": \"Postdoctoral fellow, \", \"DATE\": \"Sept 2016 - Aug 2018\"}]}, {\"EMPL\": \"Perimeter Institute\", \"TITLS\": [{\"TITL\": \"Graduate research fellow, \", \"DATE\": \"May 2011 - June 2016\"}]}]}))\n",
       "        });\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parse.RenderJSON({'WORK HISTORY': parse.cluster_lines(parse.clf_intra_sec(res_ex3, clf_lines, \\\n",
    "                                                                          clf_headers, clf_wrk), 'HEAD_WORK')})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
